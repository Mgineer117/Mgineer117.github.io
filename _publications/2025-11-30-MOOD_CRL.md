---
title: "Out of Distribution Adaptation in Offline RL via Causal Normalizing Flows"
collection: publications
category: journals
venue: 'Mathematics: Statistics and Operational Research'
paperurl: 'https://www.mdpi.com/2227-7390/13/23/3835'
date: 2025-11-30
teaser: '/files/MOOD_CRL/walker_distribution.png'
excerpt: 'We propose to learn transition dynamics and reward function using causal normalizing flow model for out-of-distribution adaptation of a policy.'
permalink: /publication/2025-11-30-MOOD_CRL
tags:
  - Offline RL
  - Causal RL
citation: 'Cho, M., &amp; Sun, C. (2025). &quot;Out of Distribution Adaptation in Offline RL via Causal Normalizing Flows.&quot; <i>Mathematics</i>, 13(23), 3835.'
mathjax: true
---



<div style="background: #ffffff; border: 1px solid #e1e4e8; border-top: 5px solid #13294B; padding: 25px; margin: 30px 0; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); line-height: 1.6;">
  <h2 style="margin-top: 0; color: #13294B; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; font-weight: 700; letter-spacing: -0.5px; display: flex; align-items: center; gap: 8px;">
    Abstract
  </h2>
  <div style="font-size: 15px; color: #24292e; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; text-align: justify;">
    We uuse causal normalizing flows (CNFs) to enable the out-of-distribution (OOD) exploration wherein the policy can be optimized without performance degradation. Specifically, we use the model to learn the transition dynamics and reward function for data generation and augmentation in offline policy learning. Given the physics-based qualitative causal graph and precollected data, we develop a <strong>model-based offline OOD-adapting causal RL (MOOD-CRL)</strong> algorithm.
  </div>
</div>


## <span style="color: #13294B;">Key Contributions</span>

* **Optimization framework**: We propose a model architecture that uses a bijective CNFs for learning transition dynamics and a reward function and design a policy optimization framework where we use online policy optimization algorithm with OOD exploration.
* **Improved performance**: We showed that our algorithm outperforms existing model architecture with significant margin and robustness.
* **Extensive ablation**: We showed ablation studies how our framework is resilient to data quality, sophstication of algorithm (e.g., REINFORCE vs PPO), and include interpretable results on OOD predictive power using discrete environments.

---

## <span style="color: #13294B;">ðŸ›  Method Overview â€” MOOD-CRL</span>

<p style="line-height: 1.6; text-align: justify;">
  The core problem we tackle is how to leverage <strong>bijective causal normalizing flows (CNFs)</strong>â€”where input and output dimensions must strictly matchâ€”to predict transition dynamics and reward functions. 
</p>

<div style="text-align: center; margin: 30px 0;">
  <img src="{{ site.baseurl }}/files/MOOD_CRL/architecture.png" alt="MOOD-CRL Architecture" style="width: 100%; max-width: 800px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); border: 1px solid #e1e4e8;">
  <p style="font-size: 13px; color: #57606a; margin-top: 10px;"><em>Figure 1: Architecture of the Causal Normalizing Flow for predicting next-state dynamics and rewards.</em></p>
</div>



<p style="line-height: 1.6; text-align: justify;">
  The challenge lies in the bijective constraint: to predict the <strong>next state \(s'\)</strong> and <strong>reward \(r\)</strong>, the model theoretically requires an input of the same dimensionality. However, these values are exactly what we aim to estimate. 
</p>

<div style="background: #f8f9fa; border-left: 4px solid #13294B; padding: 15px; margin: 20px 0; border-radius: 0 6px 6px 0;">
  <strong>Our Approach:</strong>
  <ul style="margin-top: 10px; margin-bottom: 0;">
    <li><strong>Next State (\(s'\)):</strong> We provide the <em>current state</em> (\(s\)) as the input token, utilizing the inductive bias that \(s'\) is typically similar to \(s\) in continuous MDPs.</li>
    <li><strong>Reward (\(r\)):</strong> We "void out" the reward input by setting it to <strong>zero</strong> to create a neutral starting token.</li>
  </ul>
</div>

<p style="line-height: 1.6; text-align: justify;">
  By training the CNFs to map this specific input (current state and zero-reward) to the ground-truth target (actual next state and actual reward) from the offline dataset, the flow learns to accurately model the underlying transition manifold while satisfying the bijective requirement.
</p>


## ðŸ“– BibTeX Citation

<div style="position: relative; background: #fdfdfd; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin: 20px 0; font-family: monospace;">
  <button id="copy-btn" onclick="copyBibtex()" style="position: absolute; top: 10px; right: 10px; background: #002855; color: white; border: none; border-radius: 4px; padding: 5px 12px; cursor: pointer; font-size: 13px;">Copy</button>
  <pre id="bibtex-content" style="margin: 0; white-space: pre-wrap; color: #24292e; font-size: 14px;">
@article{cho2025out,
  title={Out of Distribution Adaptation in Offline RL via Causal Normalizing Flows},
  author={Cho, Minjae and Sun, Chuangchuang},
  journal={Mathematics},
  volume={13},
  number={23},
  pages={3835},
  year={2025},
  publisher={MDPI}
}</pre>
</div>

<script>
function copyBibtex() {
  const text = document.getElementById("bibtex-content").innerText;
  const btn = document.getElementById("copy-btn");
  
  navigator.clipboard.writeText(text).then(() => {
    btn.innerText = "Copied!";
    btn.style.background = "#28a745";
    setTimeout(() => {
      btn.innerText = "Copy";
      btn.style.background = "#002855";
    }, 2000);
  });
}
</script>