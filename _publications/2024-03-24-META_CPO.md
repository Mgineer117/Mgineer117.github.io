---
title: "Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee with Differentiable Convex Programming"
collection: publications
category: conferences
venue: '(AAAI-24) The Association for the Advancement of Artificial Intelligence'
paperurl: 'https://ojs.aaai.org/index.php/AAAI/article/view/30088/31916'
codeurl: 'https://github.com/Mgineer117/Meta-CPO'
date: 2024-03-24
teaser: '/files/META_CPO/META_CPO.gif'
excerpt: 'We propose Intrinsic Reward Policy Optimization (IRPO), a novel framework leveraging a surrogate policy gradient to overcome credit assignment and sample inefficiency in sparse-reward environments.'
permalink: /publication/2024-03-24-META_CPO
tags:
  - Safe RL
  - Meta-RL
citation: 'Cho, Minjae, and Chuangchuang Sun. "Constrained meta-reinforcement learning for adaptable safety guarantee with differentiable convex programming." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 19. 2024.'
mathjax: true
---


<div style="background: #ffffff; border: 1px solid #e1e4e8; border-top: 5px solid #13294B; padding: 25px; margin: 30px 0; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); line-height: 1.6;">
  <h2 style="margin-top: 0; color: #13294B; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; font-weight: 700; letter-spacing: -0.5px; display: flex; align-items: center; gap: 8px;">
    Abstract
  </h2>
  <div style="font-size: 15px; color: #24292e; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; text-align: justify;">
    We propose <strong>Meta-Learning via Constrained Policy Optimization (Meta-CPO)</strong> to study the unique challenges of ensuring safety in non-stationary environments by solving constrained problems through the lens of the meta-learning approach (learning-to-learn).
    We employ successive convex constrained policy updates across multiple tasks with differentiable convex programming later which we use end-to-end differentiability through parameters for meta-policy update. 
  </div>
</div>


## <span style="color: #13294B;">Key Contributions</span>

* **Adaptable Safety Guarantee using RL**: We developed a policy optimization framework where the resulting policy can achieve adaptable safety guarantee to unseen tasks.
* **Improved performance**: We develop adaptable safety benchmark and evaluate our algorithm across various baselines.

---

## <span style="color: #13294B;">ðŸŽ¥ Comparative Results on Adaptation to Unseen Tasks</span>

<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center; font-weight: bold; margin-bottom: 10px; border-bottom: 2px solid #002855; padding-bottom: 10px;">
  <div>Meta-CPO (Ours)</div>
  <div>Meta-TRPO</div>
  <div>CPO</div>
</div>

<div style="margin-bottom: 30px;">
  <p style="font-weight: bold; color: #002855; margin-bottom: 5px;">Environment: Point-Button-Hazard</p>
  <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center;">
    <div>
      <img src="/files/META_CPO/Meta_CPO.gif" alt="Meta_CPO" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="/files/META_CPO/Meta_TRPO.gif" alt="Meta_TRPO" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="/files/META_CPO/CPO.gif" alt="CPO" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
  </div>
</div>

## <span style="color: #13294B;">ðŸ›  Method Overview â€” Safe Meta-Policy Optimization</span>

The core of **Meta-CPO** is a bi-level constrained optimization framework that enables end-to-end meta-training while maintaining strict safety guarantees across unseen (in-distributional) tasks.

### 1. Local Task Updates (Inner-Loop)
For each specific task $$\mathcal{T}_i$$ among given $$M$$ number of tasks $$\{\mathcal{T}_i \}_{i=1}^{M}$$, we first initialize a local learner $$\phi_i$$ with parameters of a meta-learner $$\theta$$. For each local iteration $$k$$, we solve **constrained policy optimization (CPO)** using **differentiable convex programming** as follows:

$$\phi^{k+1}_i = \text{argmax}_{\phi_i} \; g(\phi^k_i, D^{tr}_i)^\top (\phi_i - \phi^k_i)$$

$$\qquad\text{s.t. } \frac{1}{2}\|\phi_i - \phi^k_i\|^2_H \leq \delta, \quad b_{\phi_i} + a(\phi^k_i, D^{tr}_i)^\top (\phi_i - \phi^k_i) \leq 0.$$

This ensures that task-specific exploration respects both trust-region constraints and a safety constraint $$J_C(\pi) \leq h$$.



### 2. Differentiable Meta-Update (Outer-Loop)
We update the meta-learner $$\theta$$ by maximizing the average performance across $$M$$ tasks. A major challenge in meta-learning is differentiating through the local learner updates, for which we employ computational graph generated by **differentiable constrained optimization** in local task updates to easily compute the meta-gradient via the chain rule as follows:

$$\frac{dF}{d\theta} = \frac{1}{M} \sum_{i=1}^M \left( \prod_{k=0}^{K-1} \frac{d\text{Alg}_i(\phi^{k+1}_i)}{d\phi^k_i} \right) \cdot g(\phi^K_i, D^{tr}_i).$$

By differentiating through the local learner updates, our algorithm is now ready to update the meta-learner by aggregating the backpropagated gradients with projection onto a global safety satisfaction $$G(\theta) \leq 0$$ as follows:

$$\theta' = \text{argmax}_{\theta'} \; \left( \frac{dF}{d\theta} \right)^\top (\theta' - \theta)$$

$$\qquad\text{s.t. } \frac{1}{2}\|\theta' - \theta\|^2_H \leq \delta_\theta, \quad b_\theta + \left( \frac{dG}{d\theta} \right)^\top (\theta' - \theta) \leq 0.$$

This mechanism allows the policy to generalize to new (but still in-distributional) tasks with adaptable safety guarantee.

---


## ðŸ“– BibTeX Citation

<div style="position: relative; background: #fdfdfd; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin: 20px 0; font-family: monospace;">
  <button id="copy-btn" onclick="copyBibtex()" style="position: absolute; top: 10px; right: 10px; background: #002855; color: white; border: none; border-radius: 4px; padding: 5px 12px; cursor: pointer; font-size: 13px;">Copy</button>
  <pre id="bibtex-content" style="margin: 0; white-space: pre-wrap; color: #24292e; font-size: 14px;">
@inproceedings{cho2024constrained,
  title={Constrained meta-reinforcement learning for adaptable safety guarantee with differentiable convex programming},
  author={Cho, Minjae and Sun, Chuangchuang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={20975--20983},
  year={2024}
}</pre>
</div>

<script>
function copyBibtex() {
  const text = document.getElementById("bibtex-content").innerText;
  const btn = document.getElementById("copy-btn");
  
  navigator.clipboard.writeText(text).then(() => {
    btn.innerText = "Copied!";
    btn.style.background = "#28a745";
    setTimeout(() => {
      btn.innerText = "Copy";
      btn.style.background = "#002855";
    }, 2000);
  });
}
</script>