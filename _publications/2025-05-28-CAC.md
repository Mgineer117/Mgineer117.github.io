---
title: "Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking"
collection: publications
category: preprints
venue: 'arXiv preprint arXiv:2506.15700'
paperurl: 'https://arxiv.org/pdf/2506.15700?'
codeurl: 'https://github.com/Mgineer117/CAC'
date: 2025-05-28
teaser: '/files/CAC/carl.gif'
excerpt: 'We propose a contraction actor-critic (CAC) algorithm for endowing a stability guarantee to the RL-trained policies for high-dimensional and nonlinear path-tracking problems.'
permalink: /publication/2025-05-28-CAC
tags:
  - Safe RL
  - Robotics
citation: 'Cho, Minjae, Hiroyasu Tsukamoto, and Huy Trong Tran. "Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking." arXiv preprint arXiv:2506.15700 (2025).'
mathjax: true
---


<div style="background: #ffffff; border: 1px solid #e1e4e8; border-top: 5px solid #13294B; padding: 25px; margin: 30px 0; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); line-height: 1.6;">
  <h2 style="margin-top: 0; color: #13294B; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; font-weight: 700; letter-spacing: -0.5px; display: flex; align-items: center; gap: 8px;">
    Abstract
  </h2>
  <div style="font-size: 15px; color: #24292e; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; text-align: justify;">
    Exploration is crucial to reinforcement learning, yet sparse rewards make naive exploration strategies ineffective. 
    <strong>Intrinsic Reward Policy Optimization (IRPO)</strong> leverages multiple intrinsic reward functions via a surrogate policy gradient to directly optimize a policy with respect to an extrinsic reward. Our algorithm, thus, avoids existing limitations such as credit assignment, sample inefficiency, and suboptimality across discrete and continuous environments.
  </div>
</div>


## <span style="color: #13294B;">Key Contributions</span>

* **New surrogate policy gradient**: We developed a surrogate gradient computed using intrinsic rewards to collect diverse experiences while using extrinsic rewards to optimize the policy.
* **Formal and empirical analysis**: We provide extensive analysis to characterize theoretical benefits and the underlying mechanism of the surrogate gradient.
* **Improved performance**: We evaluate our algorithm across widely used discrete and continuous environments, from basic dynamics to complex locomotion.
* **Extensive ablation**: We provide five ablation studies to justify our algorithmic design and its robustness across different intrinsic rewards.

---

## <span style="color: #13294B;">ðŸŽ¥ Comparative Results on Sparse-Reward Environments</span>

<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center; font-weight: bold; margin-bottom: 10px; border-bottom: 2px solid #002855; padding-bottom: 10px;">
  <div>IRPO (Ours)</div>
  <div>HRL</div>
  <div>PPO</div>
</div>

<div style="margin-bottom: 30px;">
  <p style="font-weight: bold; color: #002855; margin-bottom: 5px;">Environment: PointMaze-v1</p>
  <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center;">
    <div>
      <img src="/files/IRPO/pointmazev1.gif" alt="IRPO PointMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="/files/HRL/pointmazev1.gif" alt="HRL PointMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="/files/PPO/pointmazev1.gif" alt="PPO PointMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
  </div>
</div>

<div style="margin-bottom: 30px;">
  <p style="font-weight: bold; color: #002855; margin-bottom: 5px;">Environment: AntMaze-v3</p>
  <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center;">
    <div>
      <img src="/files/IRPO/antmazev3.gif" alt="IRPO AntMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="/files/HRL/antmazev3.gif" alt="HRL AntMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="/files/PPO/antmazev3.gif" alt="PPO AntMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
  </div>
</div>

## <span style="color: #13294B;">ðŸ›  Method Overview â€” IRPO Gradient</span>

At the core of **IRPO** is a surrogate policy gradient that aggregates learning signals from multiple exploratory policies to directly optimize extrinsic performance.

### IRPO Surrogate Gradient
We define the IRPO gradient as:

$$\nabla J_{\mathrm{IRPO}}\big(\theta,\{\tilde{\theta}_k\}_{k=1}^K\big) := \sum_{k=1}^K \omega_k \, \nabla_\theta J_R(\tilde{\theta}_k)$$

where each exploratory policy $\pi_{\tilde{\theta}_k}$ contributes proportionally to its extrinsic performance.

### Performance-Based Weighting
The contribution of each exploratory policy is determined by a softmax weighting:

$$\omega_k := \frac{\exp(J_R(\tilde{\theta}_k)/\tau)}{\sum_{k'=1}^K \exp(J_R(\tilde{\theta}_{k'})/\tau)}, \quad \tau \in (0,1]$$

### Backpropagated Policy Gradient
The gradient propagated from each exploratory policy to the base policy is calculated via the chain rule:

$$\nabla_\theta \log \pi_{\tilde{\theta}_k}(a \mid s) := (\nabla_\theta \tilde{\theta}_k)^\top \nabla_{\tilde{\theta}_k} \log \pi_{\tilde{\theta}_k}(a \mid s)$$

---

## <span style="color: #13294B;">Visualizing the IRPO Update Mechanism</span>

We provide a visual illustration of the IRPO update mechanism using a two-dimensional parameter space $$\theta = [\theta_1, \theta_2]^\top \in \mathbb{R}^2$$ with strictly concave extrinsic and intrinsic performance objectives. Below is our considered extrinsic and intrinsic performance objectives in this analysis:

> **Objective Functions:**
> $$J(\theta) := -\Vert \theta \Vert^2_2 \quad \text{(Extrinsic)}, \; \tilde{J}_1(\theta) := -\Vert \theta - [0, -2]^\top \Vert^2_2 \quad \text{(Intrinsic)}.$$

The result of IRPO on the objectives is depicted below.

<div style="text-align: center; margin: 30px 0;">
  <img src="{{ site.baseurl }}/files/IRPO/Empirical_analysis1.svg" alt="IRPO Update Mechanism" style="width: 100%; max-width: 800px; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); border: 1px solid #e1e4e8;">
  <p style="font-size: 13px; color: #57606a; margin-top: 10px;"><em>Figure 2: Empirical analysis of the IRPO update mechanism showing the relationship between intrinsic rewards and policy convergence.</em></p>
</div>

*Figure 3: Visual illustration of IRPOâ€™s update mechanism. We used one intrinsic reward with 5 number of exploratory policy updates.*

We highlight key observations below.
* **Exploratory Policy Updates**: The exploratory policy updates (red arrows) are explicitly directed towards the intrinsic performance objective (red star), facilitating the collection of diverse experiences.
* **Surrogate Gradient Guidance**: The base policy updatesâ€”driven by the IRPO gradientâ€”guide the agent toward a region where $$N$$ exploratory updates will land precisely at the extrinsic optimum.
* **Degree of Exploration**: Increasing the number of exploratory policy updates ($$N=2$$vs$$N=5$$) increases the degree of exploration, as indicated by the increased length of the exploratory vectors.

---

## ðŸ“– BibTeX Citation

<div style="position: relative; background: #fdfdfd; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin: 20px 0; font-family: monospace;">
  <button id="copy-btn" onclick="copyBibtex()" style="position: absolute; top: 10px; right: 10px; background: #002855; color: white; border: none; border-radius: 4px; padding: 5px 12px; cursor: pointer; font-size: 13px;">Copy</button>
  <pre id="bibtex-content" style="margin: 0; white-space: pre-wrap; color: #24292e; font-size: 14px;">
@article{cho2026intrinsic,
  title   = {Intrinsic Reward Policy Optimization for Sparse-Reward Environments},
  author  = {Cho, Minjae and Tran, Huy T.},
  journal = {arXiv preprint arXiv:2601.21391},
  year    = {2026}
}</pre>
</div>

<script>
function copyBibtex() {
  const text = document.getElementById("bibtex-content").innerText;
  const btn = document.getElementById("copy-btn");
  
  navigator.clipboard.writeText(text).then(() => {
    btn.innerText = "Copied!";
    btn.style.background = "#28a745";
    setTimeout(() => {
      btn.innerText = "Copy";
      btn.style.background = "#002855";
    }, 2000);
  });
}
</script>