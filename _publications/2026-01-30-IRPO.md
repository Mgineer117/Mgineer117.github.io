---
title: "Intrinsic Reward Policy Optimization for Sparse-Reward Environments"
collection: publications
category: preprints
permalink: /publication/2026-01-30-IRPO
excerpt: 'This paper proposes a novel IRPO framework to handle exploration in environments where rewards are extremely rare.'
date: 2026-01-30
venue: 'arXiv preprint arXiv:2601.21391'
paperurl: 'https://arxiv.org/abs/2601.21391'
citation: 'Cho, M., & Tran, H. T. (2026). &quot;Intrinsic Reward Policy Optimization for Sparse-Reward Environments.&quot; <i>arXiv preprint arXiv:2601.21391</i>.'
image: '/assets/images/IRPO_thumbnail.png' # optional thumbnail
video: 'https://www.youtube.com/embed/abc123XYZ' # optional demo video
---

> ðŸ’¡ **Abstract**  
> Exploration is central to reinforcement learning, yet sparse rewards make naive exploration strategies ineffective. **Intrinsic reward policy optimization (IRPO)** leverages multiple intrinsic rewards via a surrogate policy gradient to directly optimize extrinsic performance, avoiding previous limitations of credit assignment, sample inefficiency, and suboptimality across discrete and continuous environments.

---

## ðŸ”‘ Key Contributions
- **New surrogate policy gradient**: We developed a new surrogate gradient that is computed using intrinsic rewards (to collect diverse experiences) and an extrinsic reward (to optimize a policy).  
- **Formal and empirical analysis**: We provide extensive formal and empirical analysis to characterize its theoretical benefits and provide underlying mechanism of the surrogate gradient.  
- **Improved performance**: We extensively evaluate performance of our algorithm in widely used discrete and continuous environments from plain dynamics of agent to locomotion.  
- **Extensive ablation**: We provide five ablation studies to justify our algorithmic design and its robustness to the use of different intrinsic rewards.  

---

## ðŸ›  Method Overview â€” IRPO Gradient

At the core of **IRPO** is a surrogate policy gradient that aggregates learning signals from multiple exploratory policies to directly optimize extrinsic performance.

### IRPO Surrogate Gradient
We define the IRPO gradient as:

\[
\nabla J_{\mathrm{IRPO}}\big(\theta,\{\tilde{\theta}_k\}_{k=1}^K\big) := \sum_{k=1}^K \omega_k \, \nabla_\theta J_R(\tilde{\theta}_k),
\]

where each exploratory policy \(\pi_{\tilde{\theta}_k}\) contributes proportionally to its extrinsic performance.

### Performance-Based Weighting
The contribution of each exploratory policy is determined by a softmax weighting:

\[
\omega_k := \frac{\exp(J_R(\tilde{\theta}_k)/\tau)}{\sum_{k'=1}^K \exp(J_R(\tilde{\theta}_{k'})/\tau)}, \quad \tau \in (0,1].
\]

### Backpropagated Policy Gradient
The gradient propagated from each exploratory policy to the base policy is:

\[
\nabla_\theta J_R(\tilde{\theta}_k) = \mathbb{E}_{d^{\pi_{\tilde{\theta}_k}}}\Big[ Q_R^{\pi_{\tilde{\theta}_k}}(s,a) \, \nabla_\theta \log \pi_{\tilde{\theta}_k}(a \mid s) \Big],
\]

where \(J_R\) and \(Q_R^{\pi}\) denote the extrinsic performance objective and action-value function, respectively.  

The gradient of the log-policy is computed via the chain rule:

\[
\nabla_\theta \log \pi_{\tilde{\theta}_k}(a \mid s) := (\nabla_\theta \tilde{\theta}_k)^\top \nabla_{\tilde{\theta}_k} \log \pi_{\tilde{\theta}_k}(a \mid s).
\]

### Intuition
As \(\tau \to 0^+\), the IRPO update increasingly emphasizes the best-performing exploratory policies, guiding the base policy toward regions from which a few exploratory updates produce near-optimal extrinsic performance.

---

### ðŸ“Š Formal Analysis

After \(N\) iterations, the output policy parameters satisfy:

\[
\tilde{\theta}^{(N+1)} = \arg\max_{\tilde{\theta} \in \tilde{\Gamma}_N} J_R(\tilde{\theta}),
\]

where \(\tilde{\Gamma}_N \subset \mathbb{R}^m\) is the set of policies reachable from the base policy through \(N\) exploratory updates (\(m\) = number of policy parameters).  

This formalizes IRPO as optimizing over a structured policy neighborhood. See **Remark 1** in the paper for details.

---

### ðŸ“Š Empirical Analysis
<iframe src="files/IRPO/Empirical_analysis1.pdf" width="100%" height="600px"></iframe>

---

### ðŸŽ¥ Results on Environments
<img src="https://github.com/Mgineer117/Mgineer117.github.io/files/IRPO/pointmazev1.gif" alt="PointMaze-v1 Results" width="600"/>

---

## ðŸ“– BibTeX Citation
<div class="code-block">
```bibtex
@article{cho2026intrinsic,
  title   = {Intrinsic Reward Policy Optimization for Sparse-Reward Environments},
  author  = {Cho, Minjae and Tran, Huy T.},
  journal = {arXiv preprint arXiv:2601.21391},
  year    = {2026}
}