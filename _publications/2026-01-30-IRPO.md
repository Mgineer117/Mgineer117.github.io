---
title: "Intrinsic Reward Policy Optimization for Sparse-Reward Environments"
collection: publications
category: preprints
permalink: /publication/2026-01-30-IRPO
excerpt: 'This paper proposes a novel IRPO framework to handle exploration in environments where rewards are extremely rare.'
date: 2026-01-30
venue: 'arXiv preprint arXiv:2601.21391'
paperurl: 'https://arxiv.org/abs/2601.21391'
citation: 'Cho, M., & Tran, H. T. (2026). &quot;Intrinsic Reward Policy Optimization for Sparse-Reward Environments.&quot; <i>arXiv preprint arXiv:2601.21391</i>.'
image: '/assets/images/IRPO_thumbnail.png' # optional thumbnail
video: 'https://www.youtube.com/embed/abc123XYZ' # optional demo video
---

> ðŸ’¡ **Abstract**  
> Exploration is central to reinforcement learning, yet sparse rewards make naive exploration strategies ineffective.  
> **Intrinsic reward policy optimization (IRPO)** leverages multiple intrinsic rewards via a surrogate policy gradient to directly optimize extrinsic performance, avoiding previous limitations of credit assignment, sample inefficiency, and suboptimality across discrete and continuous environments.
---

## ðŸ”‘ Key Contributions
- **New surrogate policy gradient**: We developed a new surrogate gradient that is computed using intrinsic rewards (to collect diverse experiences) and an extrinsic reward (to optimize a policy).  
- **Formal and empirical analysis**: We provide extensive formal and empirical analysis to characterize its theoretical benefits and provide underlying mechanism of the surrogate gradient.  
- **Improved performance**: We extensively evaluate performance of our algorithm in widely used discrete and continuous environments from plain dynamics of agent to locomotion.  
- **Extensive ablation**: We provide five ablation studies to justify our algorithmic design and its robustness to the use of different intrinsic rewards.  
---

## ðŸ›  Method Overview â€” IRPO Gradient

At the core of **IRPO** is a surrogate policy gradient that aggregates learning signals from multiple exploratory policies to directly optimize extrinsic performance.

### IRPO Surrogate Gradient
We define the IRPO gradient as:
\[
\nabla J_{\mathrm{IRPO}}\big(\theta,\{\tilde{\theta}_k\}_{k=1}^K\big)
\;:=\;
\sum_{k=1}^K \omega_k \, \nabla_\theta J_R(\tilde{\theta}_k),
\tag{5}
\]

where each exploratory policy \(\pi_{\tilde{\theta}_k}\) contributes proportionally to its extrinsic performance.

### Performance-Based Weighting
The contribution of each exploratory policy is determined by a softmax weighting:
\[
\omega_k
\;:=\;
\frac{\exp\!\big(J_R(\tilde{\theta}_k)/\tau\big)}
{\sum_{k'=1}^K \exp\!\big(J_R(\tilde{\theta}_{k'})/\tau\big)},
\tag{6}
\]
with temperature parameter \(\tau \in (0,1]\) controlling the sharpness of the weighting.

### Backpropagated Policy Gradient
The gradient propagated from each exploratory policy to the base policy is given by:
\[
\nabla_\theta J_R(\tilde{\theta}_k)
\;=\;
\mathbb{E}_{d^{\pi_{\tilde{\theta}_k}}}
\Big[
Q_R^{\pi_{\tilde{\theta}_k}}(s,a)
\, \nabla_\theta \log \pi_{\tilde{\theta}_k}(a \mid s)
\Big],
\tag{7}
\]

where \(J_R\) and \(Q_R^{\pi}\) denote the extrinsic performance objective and action-value function, respectively.

The gradient of the log-policy is computed via the chain rule:
\[
\nabla_\theta \log \pi_{\tilde{\theta}_k}(a \mid s)
\;:=\;
(\nabla_\theta \tilde{\theta}_k)^\top
\nabla_{\tilde{\theta}_k}
\log \pi_{\tilde{\theta}_k}(a \mid s).
\tag{8}
\]

### Intuition
As \(\tau \to 0^+\), the IRPO update increasingly emphasizes the best-performing exploratory policies. Intuitively, this guides the base policy toward parameter regions from which a small number of exploratory updates produce near-optimal policies with respect to the extrinsic reward.

### ðŸ“Š Formal Analysis

We show that IRPO implicitly solves a constrained optimization problem over a set of reachable policies. In particular, after \(N\) iterations, the output policy parameters \(\tilde{\theta}^{(N+1)}\) satisfy
\[
\tilde{\theta}^{(N+1)} = \arg\max_{\tilde{\theta} \in \tilde{\Gamma}_N} J_R(\tilde{\theta}),
\tag{18}
\]
where \(\tilde{\Gamma}_N \subset \mathbb{R}^m\) denotes the set of policies reachable from the base policy through \(N\) exploratory updates, and \(m\) is the number of policy parameters.

This result provides a formal interpretation of IRPO as optimizing over a structured policy neighborhood induced by intrinsic exploration. For further details and discussion, see **Remark 1** in the paper.

---

### ðŸ“Š Empirical Analysis
![Empirical Results](https://github.com/Mgineer117/Mgineer117.github.io/files/IRPO/Empirical_analysis1.pdf)

---

### ðŸŽ¥ Results on Environments
![PointMaze-v1 Results](https://github.com/Mgineer117/Mgineer117.github.io/files/IRPO/pointmazev1.gif)

---

## ðŸ“– BibTeX Citation
```bibtex
@article{cho2026intrinsic,
  title   = {Intrinsic Reward Policy Optimization for Sparse-Reward Environments},
  author  = {Cho, Minjae and Tran, Huy T.},
  journal = {arXiv preprint arXiv:2601.21391},
  year    = {2026}
}