---
title: "Intrinsic Reward Policy Optimization for Sparse-Reward Environments"
collection: publications
category: preprints
permalink: /publication/2026-01-30-IRPO
excerpt: 'This paper proposes a novel IRPO framework to handle exploration in environments where rewards are extremely rare.'
date: 2026-01-30
venue: 'arXiv preprint arXiv:2601.21391'
paperurl: 'https://arxiv.org/abs/2601.21391'
citation: 'Cho, M., & Tran, H. T. (2026). "Intrinsic Reward Policy Optimization for Sparse-Reward Environments." arXiv preprint arXiv:2601.21391.'
mathjax: true
---

## ðŸ’¡ **Abstract**
> Exploration is central to reinforcement learning, yet sparse rewards make naive exploration strategies ineffective. **Intrinsic reward policy optimization (IRPO)** leverages multiple intrinsic rewards via a surrogate policy gradient to directly optimize extrinsic performance, avoiding previous limitations of credit assignment, sample inefficiency, and suboptimality across discrete and continuous environments.

---

## ðŸ”‘ Key Contributions
- **New surrogate policy gradient**: We developed a surrogate gradient computed using intrinsic rewards to collect diverse experiences while using extrinsic rewards to optimize the policy.
- **Formal and empirical analysis**: We provide extensive analysis to characterize theoretical benefits and the underlying mechanism of the surrogate gradient.
- **Improved performance**: We evaluate our algorithm across widely used discrete and continuous environments, from basic dynamics to complex locomotion.
- **Extensive ablation**: We provide five ablation studies to justify our algorithmic design and its robustness across different intrinsic rewards.

---

## ðŸŽ¥ Comparative Results on Sparse-Reward Environments

<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center; font-weight: bold; margin-bottom: 10px; border-bottom: 2px solid #002855; padding-bottom: 10px;">
  <div>IRPO (Ours)</div>
  <div>HRL</div>
  <div>PPO</div>
</div>

<div style="margin-bottom: 30px;">
  <p style="font-weight: bold; color: #002855; margin-bottom: 5px;">Environment: PointMaze-v1</p>
  <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center;">
    <div>
      <img src="https://github.com/Mgineer117/Mgineer117.github.io/raw/master/files/IRPO/pointmazev1.gif" alt="IRPO PointMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="https://github.com/Mgineer117/Mgineer117.github.io/raw/master/files/HRL/pointmazev1.gif" alt="HRL PointMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="https://github.com/Mgineer117/Mgineer117.github.io/raw/master/files/PPO/pointmazev1.gif" alt="PPO PointMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
  </div>
</div>

<div style="margin-bottom: 30px;">
  <p style="font-weight: bold; color: #002855; margin-bottom: 5px;">Environment: AntMaze-v3</p>
  <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; text-align: center;">
    <div>
      <img src="https://github.com/Mgineer117/Mgineer117.github.io/raw/master/files/IRPO/antmazev3.gif" alt="IRPO AntMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="https://github.com/Mgineer117/Mgineer117.github.io/raw/master/files/HRL/antmazev3.gif" alt="HRL AntMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
    <div>
      <img src="https://github.com/Mgineer117/Mgineer117.github.io/raw/master/files/PPO/antmazev3.gif" alt="PPO AntMaze" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
    </div>
  </div>
</div>


## ðŸ›  Method Overview â€” IRPO Gradient

At the core of **IRPO** is a surrogate policy gradient that aggregates learning signals from multiple exploratory policies to directly optimize extrinsic performance.

### IRPO Surrogate Gradient
We define the IRPO gradient as:

$$\nabla J_{\mathrm{IRPO}}\big(\theta,\{\tilde{\theta}_k\}_{k=1}^K\big) := \sum_{k=1}^K \omega_k \, \nabla_\theta J_R(\tilde{\theta}_k)$$

where each exploratory policy $\pi_{\tilde{\theta}_k}$ contributes proportionally to its extrinsic performance.

### Performance-Based Weighting
The contribution of each exploratory policy is determined by a softmax weighting:

$$\omega_k := \frac{\exp(J_R(\tilde{\theta}_k)/\tau)}{\sum_{k'=1}^K \exp(J_R(\tilde{\theta}_{k'})/\tau)}, \quad \tau \in (0,1]$$

### Backpropagated Policy Gradient
The gradient propagated from each exploratory policy to the base policy is calculated via the chain rule:

$$\nabla_\theta \log \pi_{\tilde{\theta}_k}(a \mid s) := (\nabla_\theta \tilde{\theta}_k)^\top \nabla_{\tilde{\theta}_k} \log \pi_{\tilde{\theta}_k}(a \mid s)$$

---



---

## ðŸ“– BibTeX Citation

<div style="position: relative; background: #fdfdfd; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin: 20px 0; font-family: monospace;">
  <button id="copy-btn" onclick="copyBibtex()" style="position: absolute; top: 10px; right: 10px; background: #002855; color: white; border: none; border-radius: 4px; padding: 5px 12px; cursor: pointer; font-size: 13px;">Copy</button>
  <pre id="bibtex-content" style="margin: 0; white-space: pre-wrap; color: #24292e; font-size: 14px;">
@article{cho2026intrinsic,
  title   = {Intrinsic Reward Policy Optimization for Sparse-Reward Environments},
  author  = {Cho, Minjae and Tran, Huy T.},
  journal = {arXiv preprint arXiv:2601.21391},
  year    = {2026}
}</pre>
</div>

<script>
function copyBibtex() {
  const text = document.getElementById("bibtex-content").innerText;
  const btn = document.getElementById("copy-btn");
  
  navigator.clipboard.writeText(text).then(() => {
    btn.innerText = "Copied!";
    btn.style.background = "#28a745";
    setTimeout(() => {
      btn.innerText = "Copy";
      btn.style.background = "#002855";
    }, 2000);
  });
}
</script>