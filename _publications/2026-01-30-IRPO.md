---
title: "Intrinsic Reward Policy Optimization for Sparse-Reward Environments"
collection: publications
category: preprints
permalink: /publication/2026-01-30-IRPO
excerpt: 'This paper proposes a novel IRPO framework to handle exploration in environments where rewards are extremely rare.'
date: 2026-01-30
venue: 'arXiv preprint arXiv:2601.21391'
paperurl: 'https://arxiv.org/abs/2601.21391'
citation: 'Cho, M., & Tran, H. T. (2026). &quot;Intrinsic Reward Policy Optimization for Sparse-Reward Environments.&quot; <i>arXiv preprint arXiv:2601.21391</i>.'
image: '/assets/images/IRPO_thumbnail.png' # optional thumbnail
video: 'https://www.youtube.com/embed/abc123XYZ' # optional demo video
---

## ðŸ“ Abstract
> Sparse-reward environments pose a significant challenge for standard Reinforcement Learning (RL) agents.  
> In this paper, we introduce **Intrinsic Reward Policy Optimization (IRPO)**, a framework that leverages intrinsic curiosity signals to guide exploration in environments with extremely rare rewards.  

---

## ðŸ”‘ Key Contributions
- **Novel Objective Function**: We define a new surrogate objective that incorporates intrinsic curiosity.  
- **Improved Sample Efficiency**: Achieves ~20% faster convergence on the `MountainCar-v0` benchmark.  
- **Generalizable Framework**: Can be applied to other sparse-reward benchmarks and robotics tasks.

---

## ðŸ›  Method Overview

You can use LaTeX equations inline:

The total reward $R_t$ is defined as:  

$$
R_t = r_e + \eta r_i
$$

where:  
- $r_e$: extrinsic reward  
- $r_i$: intrinsic reward  
- $\eta$: scaling factor for curiosity  

### ðŸ“Š Diagram / Figure
![IRPO Framework](https://yourdomain.com/assets/images/IRPO_diagram.png)  

### ðŸŽ¥ Video Demo
<iframe width="560" height="315" src="https://www.youtube.com/embed/abc123XYZ" 
title="IRPO Demo" frameborder="0" allowfullscreen></iframe>

---

## ðŸ“– BibTeX Citation
```bibtex
@article{cho2026intrinsic,
  title={Intrinsic Reward Policy Optimization for Sparse-Reward Environments},
  author={Cho, MJ and Tran, HT},
  journal={arXiv preprint arXiv:2601.21391},
  year={2026}
}